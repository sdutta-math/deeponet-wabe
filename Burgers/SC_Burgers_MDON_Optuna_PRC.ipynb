{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "### -----------------\n",
    "### SD added\n",
    "## Used to suppress TF warnings about 'weighted_metrics' and 'sample_weights'\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "## USE the above to suppress all warning except ERROR. Do not use if debugging or prototyping\n",
    "### -----------------\n",
    "\n",
    "# import keras_tuner\n",
    "tf.keras.backend.set_floatx('float64') \n",
    "\n",
    "import optuna\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, ScalarFormatter, FormatStrFormatter\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "\n",
    "from matplotlib import animation\n",
    "matplotlib.rc('animation', html='html5')\n",
    "from IPython.display import display\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "# Plot parameters\n",
    "plt.rc('font', family='serif')\n",
    "plt.rcParams.update({'font.size': 20,\n",
    "                     'lines.linewidth': 2,\n",
    "                     'lines.markersize':10,\n",
    "                     'axes.labelsize': 16, # fontsize for x and y labels (was 10)\n",
    "                     'axes.titlesize': 20,\n",
    "                     'xtick.labelsize': 16,\n",
    "                     'ytick.labelsize': 16,\n",
    "                     'legend.fontsize': 16,\n",
    "                     'axes.linewidth': 2})\n",
    "\n",
    "import itertools\n",
    "colors = itertools.cycle(['r','g','b','m','y','c'])\n",
    "markers = itertools.cycle(['p','d','o','^','s','x',]) #'D','H','v','*'])\n",
    "\n",
    "\n",
    "# from pathlib import Path, PurePath\n",
    "# try:\n",
    "#     base_dir.exists()\n",
    "# except:\n",
    "#     curr_dir = Path().resolve()\n",
    "#     base_dir = curr_dir.parent  \n",
    "\n",
    "# data_dir = \"/work/08372/scai/ls6/deeponet-wabe-main/data/burgers1d\"\n",
    "# fig_dir  = \"/work/08372/scai/ls6/deeponet-wabe-main/figures\"\n",
    "# scripts_dir  = \"/work/08372/scai/ls6/deeponet-wabe-main/scripts\"\n",
    "# work_dir = \"/work/08372/scai/ls6/deeponet-wabe-main/Burgers\"\n",
    "# model_dir = \"/work/08372/scai/ls6/deeponet-wabe-main/model\"\n",
    "\n",
    "# if not os.path.exists(model_dir):\n",
    "#     os.mkdir(model_dir)\n",
    "\n",
    "\n",
    "# sys.path.append(r'/work/08372/scai/ls6/deeponet-wabe-main/scripts')\n",
    "# sys.path.append(r'/work/08372/scai/ls6/deeponet-wabe-main/Burgers')\n",
    "# sys.path.append(r'/work/08372/scai/ls6/deeponet-wabe-main/Burgers/functions')\n",
    "\n",
    "###------------ Added by SD\n",
    "from pathlib import Path, PurePath\n",
    "try:\n",
    "    base_dir.exists()\n",
    "except:\n",
    "    curr_dir = Path().resolve()\n",
    "    base_dir = curr_dir.parent  \n",
    "\n",
    "scripts_dir = base_dir / \"scripts\"\n",
    "work_dir = base_dir / \"Burgers\"\n",
    "data_dir = base_dir / \"Burgers\" / \"functions\"\n",
    "model_dir = work_dir / \"Saved_DON_models\"\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "\n",
    "sys.path.append(str(scripts_dir.absolute()))\n",
    "sys.path.append(str(work_dir.absolute()))\n",
    "sys.path.append(str(data_dir.absolute()))\n",
    "###------------\n",
    "\n",
    "import modified_don as don\n",
    "import burgers_exact as bg\n",
    "\n",
    "import settings_optuna_PRC as sett\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case='Train' #or 'Predict'\n",
    "\n",
    "if case == 'Predict':\n",
    "    \n",
    "    ## Replace below with appropriate timestamp of saved model\n",
    "    model_path = model_dir+'_burgers1d_'+timestamp_don\n",
    "\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "    branch_id = np.load(model_path+'/branch_id.npy')\n",
    "\n",
    "train_epochs = sett.train_epochs\n",
    "optuna_epochs = sett.optuna_epochs\n",
    "optuna_trials = sett.optuna_trials\n",
    "\n",
    "loss = sett.loss\n",
    "optimizer_str = sett.optimizer_str\n",
    "scaling = sett.scaling\n",
    "scaler_min = sett.scaler_min\n",
    "scaler_max = sett.scaler_max\n",
    "re_train_list = sett.re_train_list \n",
    "re_val_list = sett.re_val_list\n",
    "re_test_list = sett.re_test_list \n",
    "x_extent_train = sett.x_extent_train\n",
    "t_extent_train = sett.t_extent_train\n",
    "x_extent_val = sett.x_extent_val\n",
    "t_extent_val = sett.t_extent_val\n",
    "percent_branch = sett.percent_branch \n",
    "percent_trunk = sett.percent_trunk\n",
    "\n",
    "vxn = sett.vxn\n",
    "vtn = sett.vtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_grid(L, T, vxn, vtn):\n",
    "    vx = np.linspace(0,L,vxn)\n",
    "    vt = np.linspace(0,T,vtn)\n",
    "    return vx, vt\n",
    "\n",
    "def plot_bounds_1d(p1,p2,p3,L,T, label1=None, vmin1=None, vmax1=None, name=None):\n",
    "    \n",
    "    if vmin1 is None:\n",
    "        vmin1 = np.amin([p1.min(), p2.min()])\n",
    "        vmax1 = np.amax([p1.max(), p2.max()])\n",
    "\n",
    "    fig, ((ax1), (ax2), (ax3)) = plt.subplots(1,3, figsize=(20,3))\n",
    "    pcm1 = ax1.imshow(p1,cmap='jet',origin='lower',\n",
    "                      vmin=vmin1, vmax=vmax1, extent=(0,T,0,L), aspect = 0.79)\n",
    "    ax1.yaxis.set_ticks(np.arange(0,L+0.1,1))\n",
    "    fig.colorbar(pcm1,ax=ax1)\n",
    "    ax1.set_title('Truth' +'\\n'+ '%s = %.2f'%('Re', label1)+'\\n'+\n",
    "                 '%.4f<u<%.4f'%(tf.reduce_min(p1).numpy(), tf.reduce_max(p1).numpy()))\n",
    "    pcm2 = ax2.imshow(p2,cmap='jet',origin='lower',\n",
    "                      vmin=vmin1, vmax=vmax1, extent=(0,T,0,L), aspect = 0.79)\n",
    "    ax2.yaxis.set_ticks(np.arange(0,L+0.1,1))\n",
    "\n",
    "    fig.colorbar(pcm2,ax=ax2)\n",
    "    ax2.set_title('Prediction' +'\\n'+ '%s = %.2f'%('Re', label1)+'\\n'\n",
    "                 '%.4f<u<%.4f'%(tf.reduce_min(p2).numpy(), tf.reduce_max(p2).numpy()))\n",
    "    pcm3 = ax3.imshow(p3,cmap='coolwarm',origin='lower',\n",
    "                      vmin=-0.05, vmax=0.05, extent=(0,T,0,L), aspect = 0.79)\n",
    "    ax3.yaxis.set_ticks(np.arange(0,L+0.1,1))\n",
    "\n",
    "    cbar = fig.colorbar(pcm3,ax=ax3)\n",
    "    ax3.set_title('Relative Error' +'\\n'+ '%s = %.2f'%('Re', label1))\n",
    "    \n",
    "    ax1.set_ylabel('$x$',fontsize=18)\n",
    "    ax1.set_xlabel('$t$',fontsize=18)\n",
    "    ax2.set_xlabel('$t$',fontsize=18) \n",
    "    ax3.set_xlabel('$t$',fontsize=18)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "#     plt.savefig(name)\n",
    "\n",
    "def plot_spcaetime_1d(p1,p2,p3,p4,p5,p6,T,L, colormap='jet', label1=None, label2=None, vmin1=None, vmax1=None, name=None):\n",
    "    \"\"\"\n",
    "    Plot space-time 2d plots of 1D solutions\n",
    "    Row1 : Predicted, True, Error for Soln1\n",
    "    Row2 : Predicted, True, Error for Soln2\n",
    "    \"\"\"\n",
    "    f = plt.figure(figsize=(15,10))\n",
    "    gs = gridspec.GridSpec(3, 3, )\n",
    "    gs.update(wspace=0.2, hspace=0.2) # set the spacing between axes.\n",
    "\n",
    "    if vmin1 is None:\n",
    "        vmin1 = np.amin([p1.min(), p2.min(), p3.min(), p4.min()])\n",
    "        vmax1 = np.amax([p1.max(), p2.max(), p3.max(), p4.max()])\n",
    "\n",
    "    ax1 = plt.subplot(gs[0, 0]);\n",
    "    f1= ax1.imshow(p1,cmap=colormap,origin='lower',vmin=vmin1, vmax=vmax1, extent=(0,T,0,L), aspect = 0.59)\n",
    "    ax1.yaxis.set_ticks(np.arange(0,L+0.1,1)); ax1.xaxis.set_ticks(np.arange(0,T+0.1,1))\n",
    "    ax1.set_title('%s=%.2f'%('Re',label1[0]))\n",
    "\n",
    "    ax2 = plt.subplot(gs[0, 1]);\n",
    "    f2 = ax2.imshow(p2,cmap=colormap,origin='lower',vmin=vmin1, vmax=vmax1, extent=(0,T,0,L), aspect = 0.59)\n",
    "    ax2.yaxis.set_ticks(np.arange(0,L+0.1,1)); ax2.xaxis.set_ticks(np.arange(0,T+0.1,1))\n",
    "    ax2.set_title('%s=%.2f'%('Re',label1[1]))\n",
    "\n",
    "    ax3 = plt.subplot(gs[1, 0]);\n",
    "    f3= ax3.imshow(p3,cmap=colormap,origin='lower',vmin=vmin1, vmax=vmax1, extent=(0,T,0,L), aspect = 0.59)\n",
    "    ax3.set_xticklabels([]); ax3.set_yticklabels([])\n",
    "#     ax3.yaxis.set_ticks(np.arange(0,1.1,1))\n",
    "    ax3.set_title('%s=%.2f'%('Re',label1[2]))\n",
    "\n",
    "    ax4 = plt.subplot(gs[1, 1]);\n",
    "    f4 = ax4.imshow(p4,cmap=colormap,origin='lower',vmin=vmin1, vmax=vmax1, extent=(0,T,0,L), aspect = 0.59)\n",
    "    ax4.set_xticklabels([]); ax4.set_yticklabels([])\n",
    "    ax4.set_title('%s=%.2f'%('Re',label1[3]))\n",
    "    \n",
    "    ax5 = plt.subplot(gs[2, 0]);\n",
    "    f5= ax5.imshow(p5,cmap=colormap,origin='lower',vmin=vmin1, vmax=vmax1, extent=(0,T,0,L), aspect = 0.59)\n",
    "    ax5.set_xticklabels([]); ax5.set_yticklabels([])\n",
    "#     ax3.yaxis.set_ticks(np.arange(0,1.1,1))\n",
    "    ax5.set_title('%s=%.2f'%('Re',label1[4]))\n",
    "\n",
    "    ax6 = plt.subplot(gs[2, 1]);\n",
    "    f6 = ax6.imshow(p6,cmap=colormap,origin='lower',vmin=vmin1, vmax=vmax1, extent=(0,T,0,L), aspect = 0.59)\n",
    "    cbar1 = f.colorbar(f6, ax=list((ax1, ax2, ax3, ax4, ax5, ax6)),orientation='horizontal',aspect=50, pad=0.1)\n",
    "    ax6.set_xticklabels([]); ax6.set_yticklabels([])\n",
    "    ax6.set_title('%s=%.2f'%('Re',label1[5]))\n",
    "\n",
    "    ax1.set_ylabel('$x$',fontsize=18); #ax2.set_ylabel('$x$',fontsize=18);\n",
    "    ax3.set_ylabel('$x$',fontsize=18); #ax4.set_ylabel('$x$',fontsize=18);\n",
    "\n",
    "    ax5.set_xlabel('$t$',fontsize=18); ax6.set_xlabel('$t$',fontsize=18); \n",
    "    ax5.set_ylabel('$x$',fontsize=18); #ax6.set_ylabel('$x$',fontsize=18);\n",
    "#     plt.savefig(name)\n",
    "\n",
    "\n",
    "def multiple_burgers(Re_list,vxn,vx,vtn,vt,percent_branch,percent_trunk,id_branch=None):\n",
    "    \n",
    "    number_cases = len(Re_list)\n",
    "    branch_sensors = int(vxn*percent_branch)\n",
    "    trunk_sensors = int(vtn*vxn*percent_trunk)\n",
    "    \n",
    "    burgers_array = np.zeros((number_cases,vxn,vtn))\n",
    "    burgers_flatten = np.zeros((number_cases*vxn*vtn))\n",
    "    count = 0 \n",
    "    id0 = 0\n",
    "    id1 = vxn*vtn\n",
    "    \n",
    "    for Re in Re_list:\n",
    "        solution = np.zeros((vxn,vtn))\n",
    "        for ix,vxi in enumerate(vx):\n",
    "            solution[ix] = bg.true_solution(vxi,vt,Re)\n",
    "        burgers_array[count] = solution\n",
    "        burgers_flatten[id0:id1] = (solution.flatten())\n",
    "        count = count + 1\n",
    "        id0 = id1\n",
    "        id1 = id1 + vxn*vtn\n",
    "        \n",
    "    b0 = burgers_array[:,:,0] #u0\n",
    "    \n",
    "    if id_branch is not None:\n",
    "        id_b=np.zeros(np.shape(id_branch),dtype=int)\n",
    "        for i in range(len(id_branch[0])):\n",
    "            smallest=np.absolute(vx-id_branch[1][i])\n",
    "            id_b[0][i]=int(np.argmin(smallest))\n",
    "        id_b[1]=id_branch[1]    \n",
    "    else:\n",
    "        id_b = np.sort(np.random.choice(vxn, branch_sensors, replace=False))\n",
    "        b_coords = vx[id_b]\n",
    "        id_b = [id_b, b_coords]\n",
    "\n",
    "    b0_train = b0[:,id_b[0]]\n",
    "\n",
    "    \n",
    "    T, X = np.meshgrid(vt,vx)\n",
    "    for i in range(number_cases):\n",
    "        b0_vector = np.tile(np.expand_dims(b0_train[i],1),vtn*vxn).T\n",
    "        \n",
    "        id_t = np.sort(np.random.choice(vtn*vxn, trunk_sensors, replace=False))\n",
    "        \n",
    "        if i == 0:\n",
    "            b_input = b0_vector[id_t]\n",
    "            t_input = np.hstack([T.flatten()[id_t,None], X.flatten()[id_t,None]])\n",
    "            target = burgers_array[i,:,:].flatten()[id_t,None]\n",
    "\n",
    "        else:\n",
    "            b_input = np.vstack([b_input,b0_vector[id_t]])\n",
    "            t_input = np.vstack([t_input,np.hstack([T.flatten()[id_t,None], X.flatten()[id_t,None]])])\n",
    "            target = np.vstack([target,burgers_array[i,:,:].flatten()[id_t,None]])        \n",
    "    \n",
    "    return burgers_array, burgers_flatten, b_input, t_input, target, id_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re_train = [15,25,50,75,100,150,200,300,400,600,800,900]\n",
    "Re_train =  re_train_list\n",
    "L_train =  x_extent_train\n",
    "T_train =  t_extent_train\n",
    "vx, vt = define_grid(L_train, T_train, vxn, vtn)\n",
    "percent_branch =  percent_branch\n",
    "percent_trunk =  percent_trunk\n",
    "burgers_array_train, burgers_flatten_train, b_train, \\\n",
    "                    t_train, target_train, id_branch = multiple_burgers(Re_train,vxn,vx,vtn,vt,\n",
    "                                                            percent_branch,percent_trunk)\n",
    "\n",
    "if case == 'Predict':\n",
    "    id_branch=branch_id\n",
    "    \n",
    "Re_val =  re_val_list\n",
    "L_val =  x_extent_val\n",
    "T_val =  t_extent_val\n",
    "vx, vt = define_grid(L_val, T_val, vxn, vtn)\n",
    "percent_branch =  percent_branch\n",
    "percent_trunk =  percent_trunk\n",
    "burgers_array_val, burgers_flatten_val, b_val, \\\n",
    "                    t_val, target_val, _ = multiple_burgers(Re_val,vxn,vx,vtn,vt,\n",
    "                                                         percent_branch,percent_trunk,id_branch=id_branch)\n",
    "\n",
    "Re_test =  re_test_list\n",
    "L_test = L_val\n",
    "T_test = T_val\n",
    "vx, vt = define_grid(L_test, T_test, vxn, vtn)\n",
    "percent_branch = sett.percent_branch_test\n",
    "percent_trunk = sett.percent_trunk_test\n",
    "burgers_array_test, burgers_flatten_test, b_test, \\\n",
    "                    t_test, target_test, _ = multiple_burgers(Re_test,vxn,vx,vtn,vt, \n",
    "                                                              percent_branch,percent_trunk,id_branch)\n",
    "\n",
    "if  scaling is True:\n",
    "    x_scaler = MinMaxScaler(feature_range=( scaler_min,  scaler_max))\n",
    "    t_scaler = MinMaxScaler(feature_range=( scaler_min,  scaler_max))\n",
    "    u_scaler = MinMaxScaler(feature_range=( scaler_min,  scaler_max))\n",
    "    b_scaler = MinMaxScaler(feature_range=( scaler_min,  scaler_max))\n",
    "\n",
    "    x_scaler.fit(np.expand_dims(t_val[:,0],1))\n",
    "    t_scaler.fit(np.expand_dims(t_val[:,1],1))\n",
    "    u_scaler.fit(target_val)\n",
    "    b_scaler.fit(b_val)\n",
    "\n",
    "    t_train[:,0] = np.squeeze(x_scaler.transform(np.expand_dims(t_train[:,0],1)))\n",
    "    t_train[:,1] = np.squeeze(t_scaler.transform(np.expand_dims(t_train[:,1],1)))\n",
    "    b_train = b_scaler.transform(b_train)\n",
    "    target_train = u_scaler.transform(target_train)\n",
    "    \n",
    "    t_val[:,0] = np.squeeze(x_scaler.transform(np.expand_dims(t_val[:,0],1)))\n",
    "    t_val[:,1] = np.squeeze(t_scaler.transform(np.expand_dims(t_val[:,1],1)))\n",
    "    b_val = b_scaler.transform(b_val)\n",
    "    target_val = u_scaler.transform(target_val)\n",
    "    \n",
    "    t_test[:,0] = np.squeeze(x_scaler.transform(np.expand_dims(t_test[:,0],1)))\n",
    "    t_test[:,1] = np.squeeze(t_scaler.transform(np.expand_dims(t_test[:,1],1)))\n",
    "    b_test = b_scaler.transform(b_test)\n",
    "    target_test = u_scaler.transform(target_test)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(trial):\n",
    "    # Define search space\n",
    "    verbosity_mode = 1\n",
    "\n",
    "    branch_sensors = int(vxn* percent_branch)\n",
    "    neurons_layer = trial.suggest_int(\"neurons_layer\",\n",
    "                                  sett.neurons_layer_lower,\n",
    "                                  sett.neurons_layer_upper,\n",
    "                                  sett.neurons_layer_step)  \n",
    "    b_number_layers = trial.suggest_int(\"b_layers\", \n",
    "                                        sett.b_number_layers_lower, \n",
    "                                        sett.b_number_layers_upper,\n",
    "                                        sett.b_number_layers_step) \n",
    "    b_actf = trial.suggest_categorical(\"b_actf\", \n",
    "                                       sett.b_actf)  \n",
    "    b_regularizer = trial.suggest_categorical(\"b_regularizer\", \n",
    "                                              sett.b_regularizer)  \n",
    "    b_initializer = trial.suggest_categorical(\"b_initializer\", \n",
    "                                              sett.b_initializer) \n",
    "    \n",
    "    \n",
    "    encoder_neurons = trial.suggest_int(\"encoder_neurons\",\n",
    "                                  sett.neurons_layer_encoder_lower,\n",
    "                                  sett.neurons_layer_encoder_upper,\n",
    "                                  sett.neurons_layer_encoder_step) \n",
    "    b_encoder_layers = trial.suggest_int(\"b_encoderlayers\", \n",
    "                                        sett.b_number_layers_encoder_lower, \n",
    "                                        sett.b_number_layers_encoder_upper,\n",
    "                                        sett.b_number_layers_encoder_step) \n",
    "    b_encoder_actf = trial.suggest_categorical(\"b_encoder_actf\", \n",
    "                                       sett.b_encoder_actf)  \n",
    "    b_encoder_regularizer = trial.suggest_categorical(\"b_encoder_regularizer\", \n",
    "                                              sett.b_encoder_regularizer)  \n",
    "    b_encoder_init = trial.suggest_categorical(\"b_encoder_initializer\", \n",
    "                                              sett.b_encoder_init) \n",
    "    \n",
    "    t_number_layers = trial.suggest_int(\"t_layers\", \n",
    "                                        sett.t_number_layers_lower, \n",
    "                                        sett.t_number_layers_upper)     \n",
    "    t_actf = trial.suggest_categorical(\"t_actf\", \n",
    "                                       sett.t_actf)  \n",
    "    t_regularizer = trial.suggest_categorical(\"t_regularizer\", \n",
    "                                              sett.t_regularizer)  \n",
    "    t_initializer = trial.suggest_categorical(\"t_initializer\", \n",
    "                                              sett.t_initializer) \n",
    "    \n",
    "    \n",
    "    t_encoder_layers = trial.suggest_int(\"t_encoderlayers\", \n",
    "                                        sett.t_number_layers_encoder_lower, \n",
    "                                        sett.t_number_layers_encoder_upper,\n",
    "                                        sett.t_number_layers_encoder_step) \n",
    "    \n",
    "    t_encoder_actf = trial.suggest_categorical(\"t_encoder_actf\", \n",
    "                                       sett.t_encoder_actf)  \n",
    "    t_encoder_regularizer = trial.suggest_categorical(\"t_encoder_regularizer\", \n",
    "                                              sett.t_encoder_regularizer)  \n",
    "    t_encoder_init = trial.suggest_categorical(\"t_encoder_initializer\", \n",
    "                                              sett.t_encoder_init) \n",
    "\n",
    "    init_lr = trial.suggest_categorical(\"ilr\", sett.init_lr)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(init_lr)  \n",
    "\n",
    "    loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "\n",
    "#     nn = don.don_nn(branch_input_shape = branch_sensors,\n",
    "#                 branch_output_shape = branch_output_shape,\n",
    "#                 b_number_layers = b_number_layers, \n",
    "#                 b_neurons_layer = b_neurons_layer, \n",
    "#                 b_actf = b_actf, \n",
    "#                 b_init = b_initializer, \n",
    "#                 b_regularizer = b_regularizer, \n",
    "#                 trunk_input_shape = 2, \n",
    "#                 trunk_output_shape = branch_output_shape,  ### Needs to be same as branch output shape\n",
    "#                 t_number_layers = t_number_layers, \n",
    "#                 t_neurons_layer = t_neurons_layer, \n",
    "#                 t_actf = t_actf, \n",
    "#                 t_init = t_initializer, \n",
    "#                 t_regularizer = t_regularizer,\n",
    "#                )\n",
    "    nn = don.don_nn(branch_input_shape = branch_sensors,\n",
    "                branch_output_shape = neurons_layer,\n",
    "                b_number_layers = b_number_layers, \n",
    "                b_neurons_layer = neurons_layer, ## Modified by SD\n",
    "                b_actf = b_actf, \n",
    "                b_init = b_initializer, \n",
    "                b_regularizer = b_regularizer, \n",
    "                    \n",
    "                b_encoder_layers = b_encoder_layers, \n",
    "                b_encoder_neurons = encoder_neurons,\n",
    "                b_encoder_actf = b_encoder_actf, \n",
    "                b_encoder_init = b_encoder_init, \n",
    "                b_encoder_regularizer = b_encoder_regularizer, \n",
    "                    \n",
    "                trunk_input_shape = 2, \n",
    "                trunk_output_shape = neurons_layer,  ### Needs to be same as branch output shape\n",
    "                t_number_layers = t_number_layers, \n",
    "                t_neurons_layer = neurons_layer,  ## Modified by SD\n",
    "                t_actf = t_actf, \n",
    "                t_init = t_initializer, \n",
    "                t_regularizer = t_regularizer,\n",
    "                \n",
    "                t_encoder_layers = t_encoder_layers, \n",
    "                t_encoder_neurons = encoder_neurons, \n",
    "                t_encoder_actf = t_encoder_actf, \n",
    "                t_encoder_init = t_encoder_init, \n",
    "                t_encoder_regularizer = t_encoder_regularizer,  \n",
    "               )\n",
    "    \n",
    "    model = don.don_model(nn)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = optimizer,\n",
    "        loss_fn = loss_obj)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI: Objective functions can take additional arguments\n",
    "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
    "\n",
    "\n",
    "# Wrap training step for search\n",
    "def objective(trial):\n",
    "\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()\n",
    "\n",
    "    # Build model and optimizer.\n",
    "    model = NN(trial)\n",
    "\n",
    "    ## TODO::: CAN THIS BE OPTIMIZED??\n",
    "    percent_trunk = sett.percent_trunk_test\n",
    "    \n",
    "    buffer_size = int(vtn*vxn*percent_trunk*len( re_train_list))\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", sett.batch_size)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((b_train,t_train, target_train))\n",
    "    dataset = dataset.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "    \n",
    "    # batch_size = int(vtn*vxn* percent_trunk*len( re_val_list))\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((b_val,t_val, target_val))\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "    \n",
    "    history = model.fit(dataset,validation_data=val_dataset,\n",
    "                        epochs = optuna_epochs)#, callbacks=[reduce_lr])\n",
    "    \n",
    "    score = model.evaluate(val_dataset, verbose=0)\n",
    "    \n",
    "    print(score)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTUNA Trials\n",
    "\n",
    "# Define search parameters\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials = optuna_trials, timeout = 10000000000)\n",
    "\n",
    "\n",
    "#\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "sys.stdout = open(f\"burgers1d_don_optuna.txt\", \"w\")\n",
    "#\n",
    "\n",
    "# Print results\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for keyy, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(keyy, value))\n",
    "\n",
    "print(\"Importance:\")\n",
    "print(optuna.importance.get_param_importances(study))\n",
    "\n",
    "\n",
    "#\n",
    "sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "#\n",
    "\n",
    "#export DataFrame to text file (keep header row and index column)\n",
    "with open(f'burgers1d_don_trials.txt', 'a') as f:\n",
    "    df_string = study.trials_dataframe().sort_values(\"value\").to_string()\n",
    "    f.write(df_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# branch_sensors = int(vxn* percent_branch)\n",
    "\n",
    "##DON Model Training\n",
    "model = NN(trial)\n",
    "\n",
    "batch_size = trial.params['batch_size']\n",
    "\n",
    "## This definition needs to change if \"percent_trunk\" is also optimizer by Optuna\n",
    "buffer_size = int(vtn*vxn*percent_trunk*len( re_train_list))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((b_train,t_train, target_train))\n",
    "dataset = dataset.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "   \n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((b_val,t_val, target_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9,\n",
    "                          patience=sett.reduce_patience, min_lr=1e-6, min_delta=1e-10, verbose=1)\n",
    "\n",
    "don_checkpoint_filepath = './tmp/checkpoint_burgers1d_don'\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=don_checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     min_delta=1e-8,\n",
    "#     patience=500,\n",
    "#     verbose=1,\n",
    "#     restore_best_weights=True\n",
    "# )\n",
    "\n",
    "i=1\n",
    "\n",
    "timestamp_don = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "out_dir = os.path.join(model_dir, 'burgers1d_don_'+timestamp_don) \n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "    \n",
    "\n",
    "if case == 'Train':\n",
    "    \n",
    "    init_time = time.time()\n",
    "    \n",
    "    model.fit(dataset, validation_data=val_dataset, epochs= train_epochs,\n",
    "              callbacks=[reduce_lr, model_check ])  # early_stop,])  ## Removed by SD\n",
    "                 \n",
    "    \n",
    "    end_time = time.time()\n",
    "    train_time = end_time - init_time\n",
    "    hrs = int(train_time//3600); rem_time = train_time - hrs*3600\n",
    "    mins = int(rem_time//60); secs = int(rem_time%60)\n",
    "    print('Training time: %d H %d M, %d S'%(hrs,mins,secs))\n",
    "    \n",
    "    \n",
    "    model.save(out_dir,id_branch,)\n",
    "    np.savez('burgers1d_don_history_'+timestamp_don, history=model.history.history, allow_pickle=True,)\n",
    "\n",
    "if case == 'Predict':   \n",
    "    model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "lrate = model.history.history['lr']\n",
    "train_epoch = model.history.epoch\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(14,5),constrained_layout=True)\n",
    "ax[0].plot(train_epoch,train_loss,label='train_loss',marker='v',markevery=128)\n",
    "ax[0].plot(train_epoch,val_loss,label='val_loss',marker='s',markevery=128)\n",
    "\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_title('Validation and Training losses in semi-log scale')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_epoch,lrate,label='LR',marker='p',markevery=128)\n",
    "ax[1].set_yscale('log')\n",
    "\n",
    "ax[1].set_title('Learning rate decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_t = np.argmin(np.abs(vt-1.6))\n",
    "id_x = np.argmin(np.abs(vx-0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_res = model([b_test,t_test])\n",
    "\n",
    "if  scaling is True:\n",
    "    o_res = u_scaler.inverse_transform(o_res)\n",
    "\n",
    "test = np.reshape(np.array(o_res),(len( re_test_list),vxn,vtn))\n",
    "\n",
    "error = test - burgers_array_test\n",
    "\n",
    "plot_bounds_1d(burgers_array_test[0],test[0],error[0], L_test, T_test, label1= re_test_list[0], vmin1=0, vmax1=0.5,name='low_re'+str(i))\n",
    "plot_bounds_1d(burgers_array_test[-1],test[-1],error[-1], L_test, T_test, label1= re_test_list[-1], vmin1=0, vmax1=0.5, name='high_re'+str(i))\n",
    "\n",
    "test[:,:,id_t]=1000\n",
    "test[:,id_x,:]=1000\n",
    "burgers_array_test[:,:,id_t]=1000\n",
    "burgers_array_test[:,id_x,:]=1000\n",
    "error[:,:,id_t]=1000\n",
    "error[:,id_x,:]=1000\n",
    "\n",
    "plot_spcaetime_1d(test[0],test[1],\n",
    "                  test[2],test[3],\n",
    "                  test[4],test[5],\n",
    "                  T_test,L_test,label1= re_test_list,\n",
    "                  vmin1=0,\n",
    "                  vmax1=0.5,                      \n",
    "                  name='prediction'+str(i))\n",
    "\n",
    "plot_spcaetime_1d(error[0],error[1],\n",
    "                  error[2],error[3],\n",
    "                  error[4],error[5],\n",
    "                  T_test,L_test,\n",
    "                  colormap='coolwarm',label1= re_test_list,\n",
    "                  vmin1=-0.05,\n",
    "                  vmax1=0.05,\n",
    "                  name='error'+str(i))\n",
    "\n",
    "plot_spcaetime_1d(burgers_array_train[0],burgers_array_train[1],\n",
    "                  burgers_array_train[2],burgers_array_train[3],\n",
    "                  burgers_array_train[4],burgers_array_train[5],\n",
    "                  T_train,L_train,label1= re_train_list,\n",
    "                  vmin1=0,\n",
    "                  vmax1=0.5,                  \n",
    "                  name='train') \n",
    "\n",
    "plot_spcaetime_1d(burgers_array_test[0],burgers_array_test[1],\n",
    "                  burgers_array_test[2],burgers_array_test[3],\n",
    "                  burgers_array_test[4],burgers_array_test[5],\n",
    "                  T_test,L_test,label1= re_test_list,\n",
    "                  vmin1=0,\n",
    "                  vmax1=0.5,    \n",
    "                  name='truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
